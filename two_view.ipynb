{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "# Two-View 3D Reconstruction\n",
    "\n",
    "This notebook implements a simple two-view Structure from Motion approach to create a 3D reconstruction from two images (01.jpg and 02.jpg).\n",
    "\n",
    "## Setup and Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97438a18",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install open3d networkx matplotlib tqdm opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d0d7a7",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import open3d as o3d\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from glob import glob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For displaying Open3D visualizations in notebooks\n",
    "from google.colab.patches import cv2_imshow\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "def display_open3d_to_notebook(vis, width=900, height=600):\n",
    "    vis.update_renderer()\n",
    "    image = vis.capture_screen_float_buffer()\n",
    "    image_array = np.asarray(image)\n",
    "    image_array = (image_array * 255).astype(np.uint8)\n",
    "    image_array = cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR)\n",
    "    _, encoded_img = cv2.imencode('.png', image_array)\n",
    "    encoded_img = base64.b64encode(encoded_img)\n",
    "    html = f'<img src=\"data:image/png;base64,{encoded_img.decode()}\" width=\"{width}\" height=\"{height}\"/>'\n",
    "    return HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48944a78",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "First, let's load our two specific images: 01.jpg and 02.jpg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84126560",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# For Colab: Upload images\n",
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "# Create directories\n",
    "!mkdir -p data/images\n",
    "\n",
    "# Option 1: Upload images directly\n",
    "uploaded = files.upload()\n",
    "for filename in uploaded.keys():\n",
    "    if filename in ['01.jpg', '02.jpg']:\n",
    "        shutil.move(filename, f'data/images/{filename}')\n",
    "    elif filename.endswith('.txt') and 'K' in filename:\n",
    "        shutil.move(filename, f'data/K.txt')\n",
    "\n",
    "# Set the paths for our specific images\n",
    "img_path = \"data/images\"\n",
    "image_paths = [os.path.join(img_path, \"01.jpg\"), os.path.join(img_path, \"02.jpg\")]\n",
    "\n",
    "# Check if both images exist\n",
    "missing_images = [path for path in image_paths if not os.path.exists(path)]\n",
    "if missing_images:\n",
    "    print(f\"Warning: The following required images are missing: {missing_images}\")\n",
    "    print(\"Please upload 01.jpg and 02.jpg to continue.\")\n",
    "else:\n",
    "    print(f\"Found both required images: 01.jpg and 02.jpg\")\n",
    "\n",
    "# Load camera calibration matrix\n",
    "try:\n",
    "    K = np.loadtxt(\"data/K.txt\")\n",
    "    print(\"Camera intrinsic matrix loaded:\")\n",
    "    print(K)\n",
    "except:\n",
    "    print(\"No calibration matrix found. Using a default calibration.\")\n",
    "    # Default intrinsic matrix for approximately 1080p images\n",
    "    K = np.array([[1200, 0, 960], \n",
    "                  [0, 1200, 540], \n",
    "                  [0, 0, 1]])\n",
    "    np.savetxt(\"data/K.txt\", K)\n",
    "\n",
    "# Display the two images\n",
    "if len(missing_images) == 0:\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    for i, path in enumerate(image_paths):\n",
    "        plt.subplot(1, 2, i+1)\n",
    "        img = cv2.imread(path)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(img_rgb)\n",
    "        plt.title(f\"Image {i+1}: {os.path.basename(path)}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85c2077",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 2. Load Images and Extract Features\n",
    "\n",
    "Now we'll load our two images and extract feature points using SIFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e8006d",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Load our two images\n",
    "images = []\n",
    "images_rgb = []\n",
    "\n",
    "for path in image_paths:\n",
    "    img = cv2.imread(path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    images.append(img)\n",
    "    images_rgb.append(img_rgb)\n",
    "\n",
    "print(f\"Loaded {len(images)} images\")\n",
    "\n",
    "# Create SIFT detector with more aggressive parameters for denser features\n",
    "sift = cv2.SIFT_create(\n",
    "    nfeatures=0,  # No limit on number of features\n",
    "    contrastThreshold=0.04,  # Lower threshold for more features\n",
    "    edgeThreshold=10,  # Higher threshold for more features\n",
    "    sigma=1.6  # Default sigma\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd0154a",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Detect features in both images\n",
    "all_kp = []\n",
    "all_des = []\n",
    "\n",
    "for i, img_rgb in enumerate(images_rgb):\n",
    "    img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\n",
    "    img_eq = cv2.equalizeHist(img_gray)\n",
    "    \n",
    "    # Detect features at multiple scales\n",
    "    kp_standard, des_standard = sift.detectAndCompute(img_rgb, None)\n",
    "    kp_eq, des_eq = sift.detectAndCompute(img_eq, None)\n",
    "    \n",
    "    # Combine features\n",
    "    if des_standard is not None and des_eq is not None:\n",
    "        kp = kp_standard + kp_eq\n",
    "        des = np.vstack((des_standard, des_eq))\n",
    "    elif des_standard is not None:\n",
    "        kp = kp_standard\n",
    "        des = des_standard\n",
    "    elif des_eq is not None:\n",
    "        kp = kp_eq\n",
    "        des = des_eq\n",
    "    else:\n",
    "        kp = []\n",
    "        des = np.array([])\n",
    "    \n",
    "    all_kp.append(kp)\n",
    "    all_des.append(des)\n",
    "    print(f\"Image {i+1} ({os.path.basename(image_paths[i])}): Detected {len(kp)} keypoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53c2af1",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "### Visualize Detected Features\n",
    "\n",
    "Let's visualize the keypoints detected in both images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0eca61",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Display keypoints on both images\n",
    "plt.figure(figsize=(15, 7))\n",
    "for i in range(2):\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    img_with_kp = cv2.drawKeypoints(images_rgb[i], all_kp[i], None, \n",
    "                                    flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    plt.imshow(img_with_kp)\n",
    "    plt.title(f\"Image {i+1} ({os.path.basename(image_paths[i])}): {len(all_kp[i])} keypoints\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5021c53f",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 3. Match Features Between Images\n",
    "\n",
    "Now we'll match features between our two images to find correspondences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a967322",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Improved FLANN matcher setup\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "search_params = dict(checks=100)  # More checks for better accuracy\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "# Function to perform feature matching with cross-check\n",
    "def match_features(des1, des2, lowes_ratio=0.75, cross_check=True):\n",
    "    # Forward matching (img1 -> img2)\n",
    "    if des1 is None or des2 is None or len(des1) == 0 or len(des2) == 0:\n",
    "        return []\n",
    "    \n",
    "    matches12 = flann.knnMatch(des1, des2, k=2)\n",
    "    good_matches12 = []\n",
    "    for m, n in matches12:\n",
    "        if m.distance < lowes_ratio * n.distance:\n",
    "            good_matches12.append(m)\n",
    "    \n",
    "    if not cross_check:\n",
    "        return good_matches12\n",
    "    \n",
    "    # Backward matching (img2 -> img1) for cross-check\n",
    "    matches21 = flann.knnMatch(des2, des1, k=2)\n",
    "    good_matches21 = []\n",
    "    for m, n in matches21:\n",
    "        if m.distance < lowes_ratio * n.distance:\n",
    "            good_matches21.append(m)\n",
    "    \n",
    "    # Cross-check: keep matches that are consistent in both directions\n",
    "    cross_matches = []\n",
    "    for match12 in good_matches12:\n",
    "        for match21 in good_matches21:\n",
    "            # Check if match is consistent in both directions\n",
    "            if match12.queryIdx == match21.trainIdx and match12.trainIdx == match21.queryIdx:\n",
    "                cross_matches.append(match12)\n",
    "                break\n",
    "    \n",
    "    return cross_matches\n",
    "\n",
    "# Get matched points from keypoints and matches\n",
    "def get_matched_points(kp1, kp2, matches):\n",
    "    pts1 = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    pts2 = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    return pts1, pts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match features between our two images (01.jpg and 02.jpg)\n",
    "idx1, idx2 = 0, 1  # Using image indices 0 and 1 (01.jpg and 02.jpg)\n",
    "good_matches = match_features(all_des[idx1], all_des[idx2], lowes_ratio=0.75)\n",
    "\n",
    "print(f\"Found {len(good_matches)} good matches between {os.path.basename(image_paths[0])} and {os.path.basename(image_paths[1])}\")\n",
    "\n",
    "# Display matches for verification\n",
    "match_img = cv2.drawMatches(\n",
    "    images_rgb[idx1], all_kp[idx1], \n",
    "    images_rgb[idx2], all_kp[idx2], \n",
    "    good_matches[:100], None, \n",
    "    flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    ")\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(match_img)\n",
    "plt.title(f'Matches between {os.path.basename(image_paths[0])} and {os.path.basename(image_paths[1])}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40bc56b",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 4. Two-View Reconstruction\n",
    "\n",
    "Now we'll use the matched features to perform 3D reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613ab442",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Initialize camera poses (R|t) for each image\n",
    "# First camera (01.jpg) is at origin with identity rotation\n",
    "camera_poses = [np.hstack((np.eye(3), np.zeros((3, 1))))]\n",
    "camera_matrices = [K @ camera_poses[0]]\n",
    "\n",
    "# Structure to store 3D points and their 2D observations\n",
    "point_cloud = []\n",
    "point_colors = []\n",
    "\n",
    "pts1, pts2 = get_matched_points(all_kp[idx1], all_kp[idx2], good_matches)\n",
    "\n",
    "# Calculate essential matrix with robust parameters\n",
    "E, mask = cv2.findEssentialMat(pts1, pts2, K, method=cv2.RANSAC, prob=0.999, threshold=2.0)\n",
    "\n",
    "# Recover pose for second camera (02.jpg)\n",
    "_, R, t, mask = cv2.recoverPose(E, pts1, pts2, K, mask=mask)\n",
    "\n",
    "# Set second camera pose\n",
    "camera_poses.append(np.hstack((R, t)))\n",
    "camera_matrices.append(K @ camera_poses[1])\n",
    "\n",
    "print(f\"Camera 1 (01.jpg) pose:\\n{camera_poses[0]}\")\n",
    "print(f\"\\nCamera 2 (02.jpg) pose:\\n{camera_poses[1]}\")\n",
    "\n",
    "# Filter points using mask\n",
    "mask = mask.ravel() == 1\n",
    "pts1_good = pts1[mask]\n",
    "pts2_good = pts2[mask]\n",
    "\n",
    "print(f\"\\nUsing {np.sum(mask)} inlier matches for triangulation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33257416",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Triangulate points\n",
    "pts1_good = pts1_good.reshape(-1, 2).T\n",
    "pts2_good = pts2_good.reshape(-1, 2).T\n",
    "points_4D = cv2.triangulatePoints(camera_matrices[0], camera_matrices[1], pts1_good, pts2_good)\n",
    "points_3D = points_4D / points_4D[3]  # Convert to Cartesian\n",
    "points_3D = points_3D[:3, :].T\n",
    "\n",
    "# Filter points by depth\n",
    "valid_points = []\n",
    "valid_colors = []\n",
    "\n",
    "for i in range(points_3D.shape[0]):\n",
    "    point = points_3D[i]\n",
    "    # Check if point is in front of both cameras\n",
    "    if point[2] > 0:\n",
    "        # Get corresponding 2D points\n",
    "        x1, y1 = pts1_good[:, i]\n",
    "        \n",
    "        # Extract color from first image\n",
    "        x1_int, y1_int = int(round(x1)), int(round(y1))\n",
    "        if 0 <= x1_int < images_rgb[idx1].shape[1] and 0 <= y1_int < images_rgb[idx1].shape[0]:\n",
    "            color = images_rgb[idx1][y1_int, x1_int] / 255.0\n",
    "            valid_points.append(point)\n",
    "            valid_colors.append(color)\n",
    "\n",
    "point_cloud = np.array(valid_points)\n",
    "point_colors = np.array(valid_colors)\n",
    "\n",
    "print(f\"Reconstructed point cloud has {len(point_cloud)} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a21ecfe",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "### Visualize Point Cloud\n",
    "Let's visualize the 3D point cloud reconstructed from our two images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb564960",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Create Open3D point cloud for visualization\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(point_cloud)\n",
    "pcd.colors = o3d.utility.Vector3dVector(point_colors)\n",
    "\n",
    "# Remove outliers for cleaner visualization\n",
    "pcd, _ = pcd.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)\n",
    "\n",
    "# Create coordinate frame and camera frustums\n",
    "coordinate_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.5, origin=[0, 0, 0])\n",
    "camera_frustums = []\n",
    "\n",
    "for i in range(2):\n",
    "    pose = camera_poses[i]\n",
    "    size = 0.2\n",
    "    frustum = o3d.geometry.LineSet()\n",
    "    \n",
    "    R = pose[:, :3]\n",
    "    t = pose[:, 3]\n",
    "    center = -R.T @ t\n",
    "    \n",
    "    points = [\n",
    "        center,  # Camera center\n",
    "        center + size * R.T @ np.array([1, 1, 2]),  # Top-right\n",
    "        center + size * R.T @ np.array([-1, 1, 2]),  # Top-left\n",
    "        center + size * R.T @ np.array([-1, -1, 2]),  # Bottom-left\n",
    "        center + size * R.T @ np.array([1, -1, 2])   # Bottom-right\n",
    "    ]\n",
    "    \n",
    "    lines = [\n",
    "        [0, 1], [0, 2], [0, 3], [0, 4],  # Lines from center\n",
    "        [1, 2], [2, 3], [3, 4], [4, 1]   # Base square\n",
    "    ]\n",
    "    \n",
    "    frustum.points = o3d.utility.Vector3dVector(points)\n",
    "    frustum.lines = o3d.utility.Vector2iVector(lines)\n",
    "    \n",
    "    # Color cameras differently\n",
    "    color = [1.0, 0.0, 0.0] if i == 0 else [0.0, 0.0, 1.0]  # Red for 01.jpg, blue for 02.jpg\n",
    "    colors = [color for _ in range(len(lines))]\n",
    "    frustum.colors = o3d.utility.Vector3dVector(colors)\n",
    "    camera_frustums.append(frustum)\n",
    "\n",
    "# Visualize\n",
    "vis = o3d.visualization.Visualizer()\n",
    "vis.create_window(width=800, height=600)\n",
    "vis.add_geometry(pcd)\n",
    "vis.add_geometry(coordinate_frame)\n",
    "for frustum in camera_frustums:\n",
    "    vis.add_geometry(frustum)\n",
    "\n",
    "# Set white background\n",
    "opt = vis.get_render_option()\n",
    "opt.background_color = np.array([1.0, 1.0, 1.0])\n",
    "\n",
    "# Adjust view\n",
    "view_ctl = vis.get_view_control()\n",
    "view_ctl.set_zoom(0.8)\n",
    "\n",
    "# Show visualization\n",
    "display_open3d_to_notebook(vis)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "two_view.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
