{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "# Two-View 3D Reconstruction\n",
    "\n",
    "This notebook implements a simple two-view Structure from Motion approach to create a 3D reconstruction from two images (01.jpg and 02.jpg).\n",
    "\n",
    "## Setup and Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97438a18",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install open3d networkx matplotlib tqdm opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d0d7a7",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import open3d as o3d\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from glob import glob\n",
    "import plotly.graph_objects as go\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For displaying Open3D visualizations in notebooks\n",
    "from google.colab.patches import cv2_imshow\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "def display_open3d_to_notebook(vis, width=900, height=600):\n",
    "    vis.update_renderer()\n",
    "    image = vis.capture_screen_float_buffer()\n",
    "    image_array = np.asarray(image)\n",
    "    image_array = (image_array * 255).astype(np.uint8)\n",
    "    image_array = cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR)\n",
    "    _, encoded_img = cv2.imencode('.png', image_array)\n",
    "    encoded_img = base64.b64encode(encoded_img)\n",
    "    html = f'<img src=\"data:image/png;base64,{encoded_img.decode()}\" width=\"{width}\" height=\"{height}\"/>'\n",
    "    return HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48944a78",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "First, let's load our two specific images: 01.jpg and 02.jpg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir images\n",
    "!wget -P /content/images https://raw.githubusercontent.com/VarunBurde/Prague_ml/main/images/01.jpg\n",
    "!wget -P /content/images https://raw.githubusercontent.com/VarunBurde/Prague_ml/main/images/02.jpg\n",
    "!wget -O /content/K.txt https://raw.githubusercontent.com/VarunBurde/Prague_ml/main/K.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84126560",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# For Colab: Upload images\n",
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "# Set the paths for our specific images\n",
    "img_path = \"/content/images\"\n",
    "image_paths = [os.path.join(img_path, \"01.jpg\"), os.path.join(img_path, \"02.jpg\")]\n",
    "K = np.loadtxt(\"/content/K.txt\")\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "for i, path in enumerate(image_paths):\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    img = cv2.imread(path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.title(f\"Image {i+1}: {os.path.basename(path)}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85c2077",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 2. Load Images and Extract Features\n",
    "\n",
    "Now we'll load our two images and extract feature points using SIFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e8006d",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Create feature detectors with optimized parameters\n",
    "sift = cv2.SIFT_create(\n",
    "    nfeatures=5000,  # Increased from 0\n",
    "    contrastThreshold=0.02,  # Lowered for more features\n",
    "    edgeThreshold=20,  # Increased for more edge features\n",
    "    sigma=1.6\n",
    ")\n",
    "\n",
    "# Add ORB detector for additional features\n",
    "orb = cv2.ORB_create(\n",
    "    nfeatures=5000,\n",
    "    scaleFactor=1.2,\n",
    "    nlevels=8,\n",
    "    edgeThreshold=31,\n",
    "    firstLevel=0,\n",
    "    WTA_K=2,\n",
    "    patchSize=31\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd0154a",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def extract_features(img_rgb):\n",
    "    \"\"\"Extract features using multiple detectors and image processing\"\"\"\n",
    "    img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\n",
    "    img_eq = cv2.equalizeHist(img_gray)\n",
    "    img_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8)).apply(img_gray)\n",
    "\n",
    "    # SIFT features from different image versions\n",
    "    kp1, des1 = sift.detectAndCompute(img_gray, None)\n",
    "    kp2, des2 = sift.detectAndCompute(img_eq, None)\n",
    "    kp3, des3 = sift.detectAndCompute(img_clahe, None)\n",
    "\n",
    "    # ORB features\n",
    "    kp4, des4 = orb.detectAndCompute(img_gray, None)\n",
    "\n",
    "    # Combine all features\n",
    "    keypoints = kp1 + kp2 + kp3 + kp4\n",
    "    descriptors = []\n",
    "\n",
    "    if des1 is not None:\n",
    "        descriptors.append(des1)\n",
    "    if des2 is not None:\n",
    "        descriptors.append(des2)\n",
    "    if des3 is not None:\n",
    "        descriptors.append(des3)\n",
    "    if des4 is not None:\n",
    "        # Convert ORB descriptors to float32 for compatibility\n",
    "        des4 = np.float32(des4)\n",
    "        if des4.shape[1] != 128:  # SIFT descriptor length\n",
    "            des4 = cv2.resize(des4, (128, des4.shape[0]))\n",
    "        descriptors.append(des4)\n",
    "\n",
    "    if descriptors:\n",
    "        descriptors = np.vstack(descriptors)\n",
    "    else:\n",
    "        descriptors = np.array([])\n",
    "\n",
    "    return keypoints, descriptors\n",
    "\n",
    "# Extract features for all images\n",
    "all_kp = []\n",
    "all_des = []\n",
    "\n",
    "for i, img_rgb in enumerate(images_rgb):\n",
    "    kp, des = extract_features(img_rgb)\n",
    "    all_kp.append(kp)\n",
    "    all_des.append(des)\n",
    "    print(f\"Image {i+1} ({os.path.basename(image_paths[i])}): Detected {len(kp)} keypoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53c2af1",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "### Visualize Detected Features\n",
    "\n",
    "Let's visualize the keypoints detected in both images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0eca61",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Display keypoints on both images\n",
    "plt.figure(figsize=(15, 7))\n",
    "for i in range(2):\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    img_with_kp = cv2.drawKeypoints(images_rgb[i], all_kp[i], None, \n",
    "                                    flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    plt.imshow(img_with_kp)\n",
    "    plt.title(f\"Image {i+1} ({os.path.basename(image_paths[i])}): {len(all_kp[i])} keypoints\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5021c53f",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 3. Match Features Between Images\n",
    "\n",
    "Now we'll match features between our two images to find correspondences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a967322",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Improved FLANN matcher setup\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "search_params = dict(checks=100)  # More checks for better accuracy\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "# Function to perform feature matching with cross-check\n",
    "def match_features(des1, des2, lowes_ratio=0.75, cross_check=True):\n",
    "    # Forward matching (img1 -> img2)\n",
    "    if des1 is None or des2 is None or len(des1) == 0 or len(des2) == 0:\n",
    "        return []\n",
    "    \n",
    "    matches12 = flann.knnMatch(des1, des2, k=2)\n",
    "    good_matches12 = []\n",
    "    for m, n in matches12:\n",
    "        if m.distance < lowes_ratio * n.distance:\n",
    "            good_matches12.append(m)\n",
    "    \n",
    "    if not cross_check:\n",
    "        return good_matches12\n",
    "    \n",
    "    # Backward matching (img2 -> img1) for cross-check\n",
    "    matches21 = flann.knnMatch(des2, des1, k=2)\n",
    "    good_matches21 = []\n",
    "    for m, n in matches21:\n",
    "        if m.distance < lowes_ratio * n.distance:\n",
    "            good_matches21.append(m)\n",
    "    \n",
    "    # Cross-check: keep matches that are consistent in both directions\n",
    "    cross_matches = []\n",
    "    for match12 in good_matches12:\n",
    "        for match21 in good_matches21:\n",
    "            # Check if match is consistent in both directions\n",
    "            if match12.queryIdx == match21.trainIdx and match12.trainIdx == match21.queryIdx:\n",
    "                cross_matches.append(match12)\n",
    "                break\n",
    "    \n",
    "    return cross_matches\n",
    "\n",
    "# Get matched points from keypoints and matches\n",
    "def get_matched_points(kp1, kp2, matches):\n",
    "    pts1 = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    pts2 = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    return pts1, pts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match features between our two images (01.jpg and 02.jpg)\n",
    "idx1, idx2 = 0, 1  # Using image indices 0 and 1 (01.jpg and 02.jpg)\n",
    "good_matches = match_features(all_des[idx1], all_des[idx2], lowes_ratio=0.75)\n",
    "\n",
    "print(f\"Found {len(good_matches)} good matches between {os.path.basename(image_paths[0])} and {os.path.basename(image_paths[1])}\")\n",
    "\n",
    "# Display matches for verification\n",
    "match_img = cv2.drawMatches(\n",
    "    images_rgb[idx1], all_kp[idx1], \n",
    "    images_rgb[idx2], all_kp[idx2], \n",
    "    good_matches[:100], None, \n",
    "    flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    ")\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(match_img)\n",
    "plt.title(f'Matches between {os.path.basename(image_paths[0])} and {os.path.basename(image_paths[1])}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40bc56b",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 4. Two-View Reconstruction\n",
    "\n",
    "Now we'll use the matched features to perform 3D reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613ab442",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Initialize camera poses (R|t) for each image\n",
    "# First camera (01.jpg) is at origin with identity rotation\n",
    "camera_poses = [np.hstack((np.eye(3), np.zeros((3, 1))))]\n",
    "camera_matrices = [K @ camera_poses[0]]\n",
    "\n",
    "# Structure to store 3D points and their 2D observations\n",
    "point_cloud = []\n",
    "point_colors = []\n",
    "\n",
    "pts1, pts2 = get_matched_points(all_kp[idx1], all_kp[idx2], good_matches)\n",
    "\n",
    "# Calculate essential matrix with robust parameters\n",
    "E, mask = cv2.findEssentialMat(pts1, pts2, K, method=cv2.RANSAC, prob=0.999, threshold=2.0)\n",
    "\n",
    "# Recover pose for second camera (02.jpg)\n",
    "_, R, t, mask = cv2.recoverPose(E, pts1, pts2, K, mask=mask)\n",
    "\n",
    "# Set second camera pose\n",
    "camera_poses.append(np.hstack((R, t)))\n",
    "camera_matrices.append(K @ camera_poses[1])\n",
    "\n",
    "print(f\"Camera 1 (01.jpg) pose:\\n{camera_poses[0]}\")\n",
    "print(f\"\\nCamera 2 (02.jpg) pose:\\n{camera_poses[1]}\")\n",
    "\n",
    "# Filter points using mask\n",
    "mask = mask.ravel() == 1\n",
    "pts1_good = pts1[mask]\n",
    "pts2_good = pts2[mask]\n",
    "\n",
    "print(f\"\\nUsing {np.sum(mask)} inlier matches for triangulation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33257416",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Triangulate points\n",
    "pts1_good = pts1_good.reshape(-1, 2).T\n",
    "pts2_good = pts2_good.reshape(-1, 2).T\n",
    "points_4D = cv2.triangulatePoints(camera_matrices[0], camera_matrices[1], pts1_good, pts2_good)\n",
    "points_3D = points_4D / points_4D[3]  # Convert to Cartesian\n",
    "points_3D = points_3D[:3, :].T\n",
    "\n",
    "# Filter points by depth\n",
    "valid_points = []\n",
    "valid_colors = []\n",
    "\n",
    "for i in range(points_3D.shape[0]):\n",
    "    point = points_3D[i]\n",
    "    # Check if point is in front of both cameras\n",
    "    if point[2] > 0:\n",
    "        # Get corresponding 2D points\n",
    "        x1, y1 = pts1_good[:, i]\n",
    "        \n",
    "        # Extract color from first image\n",
    "        x1_int, y1_int = int(round(x1)), int(round(y1))\n",
    "        if 0 <= x1_int < images_rgb[idx1].shape[1] and 0 <= y1_int < images_rgb[idx1].shape[0]:\n",
    "            color = images_rgb[idx1][y1_int, x1_int] / 255.0\n",
    "            valid_points.append(point)\n",
    "            valid_colors.append(color)\n",
    "\n",
    "point_cloud = np.array(valid_points)\n",
    "point_colors = np.array(valid_colors)\n",
    "\n",
    "print(f\"Reconstructed point cloud has {len(point_cloud)} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a21ecfe",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "### Visualize Point Cloud\n",
    "Let's visualize the 3D point cloud reconstructed from our two images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb564960",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Create Open3D point cloud for visualization\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(point_cloud)\n",
    "pcd.colors = o3d.utility.Vector3dVector(point_colors)\n",
    "\n",
    "# Remove outliers for cleaner visualization\n",
    "pcd, _ = pcd.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_geometries(geometries):\n",
    "    graph_objects = []\n",
    "\n",
    "    for geometry in geometries:\n",
    "        geometry_type = geometry.get_geometry_type()\n",
    "\n",
    "        if geometry_type == o3d.geometry.Geometry.Type.PointCloud:\n",
    "            points = np.asarray(geometry.points)\n",
    "            colors = None\n",
    "            if geometry.has_colors():\n",
    "                colors = np.asarray(geometry.colors)\n",
    "            elif geometry.has_normals():\n",
    "                colors = (0.5, 0.5, 0.5) + np.asarray(geometry.normals) * 0.5\n",
    "            else:\n",
    "                geometry.paint_uniform_color((1.0, 0.0, 0.0))\n",
    "                colors = np.asarray(geometry.colors)\n",
    "\n",
    "            scatter_3d = go.Scatter3d(x=points[:,0], y=points[:,1], z=points[:,2], mode='markers', marker=dict(size=1, color=colors))\n",
    "            graph_objects.append(scatter_3d)\n",
    "\n",
    "        if geometry_type == o3d.geometry.Geometry.Type.TriangleMesh:\n",
    "            triangles = np.asarray(geometry.triangles)\n",
    "            vertices = np.asarray(geometry.vertices)\n",
    "            colors = None\n",
    "            if geometry.has_triangle_normals():\n",
    "                colors = (0.5, 0.5, 0.5) + np.asarray(geometry.triangle_normals) * 0.5\n",
    "                colors = tuple(map(tuple, colors))\n",
    "            else:\n",
    "                colors = (1.0, 0.0, 0.0)\n",
    "\n",
    "            mesh_3d = go.Mesh3d(x=vertices[:,0], y=vertices[:,1], z=vertices[:,2], i=triangles[:,0], j=triangles[:,1], k=triangles[:,2], facecolor=colors, opacity=0.50)\n",
    "            graph_objects.append(mesh_3d)\n",
    "\n",
    "    fig = go.Figure(\n",
    "        data=graph_objects,\n",
    "        layout=dict(\n",
    "            scene=dict(\n",
    "                xaxis=dict(visible=False),\n",
    "                yaxis=dict(visible=False),\n",
    "                zaxis=dict(visible=False)\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries = draw_geometries # replace function\n",
    "o3d.visualization.draw_geometries([pcd])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-View Reconstruction\n",
    "\n",
    "Now let's extend our reconstruction to handle multiple views and compare with the two-view result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download additional images\n",
    "!wget -P /content/images https://raw.githubusercontent.com/VarunBurde/Prague_ml/main/images/03.jpg\n",
    "!wget -P /content/images https://raw.githubusercontent.com/VarunBurde/Prague_ml/main/images/04.jpg\n",
    "\n",
    "# Update image paths for multi-view\n",
    "multi_image_paths = sorted(glob(os.path.join(img_path, \"*.jpg\")))\n",
    "print(f\"Total images for multi-view reconstruction: {len(multi_image_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triangulate_points(P1, P2, pts1, pts2):\n",
    "    \"\"\"Triangulate 3D points from 2D correspondences\"\"\"\n",
    "    # Ensure points are 2xN\n",
    "    pts1 = np.ascontiguousarray(pts1.reshape(-1, 2).T)\n",
    "    pts2 = np.ascontiguousarray(pts2.reshape(-1, 2).T)\n",
    "\n",
    "    # Triangulate\n",
    "    points_4d = cv2.triangulatePoints(P1, P2, pts1, pts2)\n",
    "    points_3d = (points_4d[:3] / points_4d[3]).T\n",
    "\n",
    "    return points_3d\n",
    "\n",
    "def check_pose_scale(R, t, pts1, pts2, K):\n",
    "    \"\"\"Check if recovered pose has reasonable scale\"\"\"\n",
    "    P1 = K @ np.hstack((np.eye(3), np.zeros((3, 1))))\n",
    "    P2 = K @ np.hstack((R, t))\n",
    "    \n",
    "    try:\n",
    "        points_3d = triangulate_points(P1, P2, pts1, pts2)\n",
    "        depths = points_3d[:, 2]\n",
    "        if len(depths[depths > 0]) == 0:\n",
    "            return False\n",
    "            \n",
    "        median_depth = np.median(depths[depths > 0])\n",
    "        point_spread = np.std(points_3d, axis=0)\n",
    "        \n",
    "        return 0.1 < median_depth < 100 and np.all(point_spread < 50)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def refine_pose(R, t, pts1, pts2, K):\n",
    "    \"\"\"Refine pose estimation using PnP\"\"\"\n",
    "    try:\n",
    "        P1 = K @ np.hstack((np.eye(3), np.zeros((3, 1))))\n",
    "        P2 = K @ np.hstack((R, t))\n",
    "        points_3d = triangulate_points(P1, P2, pts1, pts2)\n",
    "\n",
    "        # Use PnP to refine pose\n",
    "        success, R_vec, t_refined, inliers = cv2.solvePnPRansac(\n",
    "            points_3d,\n",
    "            pts2.reshape(-1, 1, 2),\n",
    "            K,\n",
    "            None,\n",
    "            flags=cv2.SOLVEPNP_ITERATIVE,\n",
    "            confidence=0.99,\n",
    "            reprojectionError=8.0\n",
    "        )\n",
    "\n",
    "        if not success:\n",
    "            return R, t, None\n",
    "\n",
    "        R_refined, _ = cv2.Rodrigues(R_vec)\n",
    "        return R_refined, t_refined, inliers\n",
    "    except:\n",
    "        return R, t, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_relative_pose(pts1, pts2, K, min_inliers=15):\n",
    "    \"\"\"Improved relative pose estimation\"\"\"\n",
    "    if pts1.shape[0] < min_inliers:\n",
    "        return None, None, None, 0\n",
    "\n",
    "    # Try multiple RANSAC thresholds\n",
    "    thresholds = [0.001, 0.005, 0.01, 0.02]\n",
    "    best_result = None\n",
    "    best_inliers = 0\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        try:\n",
    "            # Normalize points\n",
    "            pts1_norm = cv2.undistortPoints(pts1.reshape(-1, 1, 2), K, None)\n",
    "            pts2_norm = cv2.undistortPoints(pts2.reshape(-1, 1, 2), K, None)\n",
    "\n",
    "            # Estimate essential matrix\n",
    "            E, mask = cv2.findEssentialMat(\n",
    "                pts1_norm,\n",
    "                pts2_norm,\n",
    "                np.eye(3),\n",
    "                method=cv2.RANSAC,\n",
    "                prob=0.999,\n",
    "                threshold=threshold\n",
    "            )\n",
    "\n",
    "            if E is None:\n",
    "                continue\n",
    "\n",
    "            # Recover pose\n",
    "            _, R, t, mask = cv2.recoverPose(E, pts1_norm, pts2_norm, np.eye(3), mask=mask)\n",
    "            inliers = np.sum(mask)\n",
    "\n",
    "            if inliers > best_inliers:\n",
    "                best_inliers = inliers\n",
    "                best_result = (R, t, mask, inliers)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in pose estimation: {e}\")\n",
    "            continue\n",
    "\n",
    "    if best_result is None:\n",
    "        return None, None, None, 0\n",
    "\n",
    "    R, t, mask, inliers = best_result\n",
    "    \n",
    "    # Refine pose\n",
    "    good_pts1 = pts1[mask.ravel()==1]\n",
    "    good_pts2 = pts2[mask.ravel()==1]\n",
    "    R_refined, t_refined, _ = refine_pose(R, t, good_pts1, good_pts2, K)\n",
    "\n",
    "    return R_refined, t_refined, mask, inliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_pair(img1_idx, img2_idx, all_images, all_descriptors, all_keypoints, K, camera_poses, camera_matrices=None):\n",
    "    \"\"\"Process a pair of images for reconstruction\"\"\"\n",
    "    if img1_idx >= len(camera_poses):\n",
    "        print(f\"Warning: Camera pose not available for image {img1_idx}\")\n",
    "        return None, None, None, -1\n",
    "\n",
    "    best_pose = None\n",
    "    best_points = None\n",
    "    best_colors = None\n",
    "    best_score = -1\n",
    "\n",
    "    # Get camera matrix for first image\n",
    "    P1 = K @ camera_poses[img1_idx]\n",
    "\n",
    "    # Try different matching parameters\n",
    "    for ratio in [0.8, 0.85, 0.75]:\n",
    "        try:\n",
    "            matches = match_features(all_descriptors[img1_idx], all_descriptors[img2_idx], lowes_ratio=ratio)\n",
    "            if len(matches) < 100:  # Increased minimum matches requirement\n",
    "                continue\n",
    "\n",
    "            pts1, pts2 = get_matched_points(all_keypoints[img1_idx], all_keypoints[img2_idx], matches)\n",
    "            R, t, mask, inliers = estimate_relative_pose(pts1, pts2, K)\n",
    "\n",
    "            if R is None:\n",
    "                continue\n",
    "\n",
    "            # Convert to global coordinates\n",
    "            R_prev = camera_poses[img1_idx][:, :3]\n",
    "            t_prev = camera_poses[img1_idx][:, 3:]\n",
    "            R_new = R @ R_prev\n",
    "            t_new = R @ t_prev + t\n",
    "\n",
    "            # Create new camera matrix\n",
    "            P2 = K @ np.hstack((R_new, t_new))\n",
    "\n",
    "            # Triangulate points\n",
    "            good_pts1 = pts1[mask.ravel()==1]\n",
    "            good_pts2 = pts2[mask.ravel()==1]\n",
    "            points_3d = triangulate_points(P1, P2, good_pts1, good_pts2)\n",
    "\n",
    "            # Score this pose\n",
    "            valid_points = points_3d[points_3d[:, 2] > 0]\n",
    "            if len(valid_points) < 50:  # Increased minimum points requirement\n",
    "                continue\n",
    "                \n",
    "            score = inliers * (1 - np.mean(np.abs(valid_points[:, 2])))\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_pose = np.hstack((R_new, t_new))\n",
    "                best_points = points_3d\n",
    "                \n",
    "                # Get colors from first image\n",
    "                colors = []\n",
    "                for pt, (x, y) in zip(points_3d, good_pts1):\n",
    "                    if 0 <= int(y) < all_images[img1_idx].shape[0] and 0 <= int(x) < all_images[img1_idx].shape[1]:\n",
    "                        color = all_images[img1_idx][int(y), int(x)] / 255.0\n",
    "                        colors.append(color)\n",
    "                best_colors = np.array(colors)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image pair {img1_idx}-{img2_idx}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return best_pose, best_points, best_colors, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_multiview(image_paths, K):\n",
    "    \"\"\"Multi-view reconstruction pipeline\"\"\"\n",
    "    # Initialize variables\n",
    "    all_images = []\n",
    "    all_keypoints = []\n",
    "    all_descriptors = []\n",
    "    camera_poses = []\n",
    "    camera_matrices = []\n",
    "    points_3d = []\n",
    "    point_colors = []\n",
    "\n",
    "    # Load and extract features\n",
    "    for path in image_paths:\n",
    "        img = cv2.imread(path)\n",
    "        if img is None:\n",
    "            print(f\"Failed to load image: {path}\")\n",
    "            continue\n",
    "            \n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        kp, des = extract_features(img_rgb)\n",
    "        \n",
    "        all_images.append(img_rgb)\n",
    "        all_keypoints.append(kp)\n",
    "        all_descriptors.append(des)\n",
    "        print(f\"Extracted {len(kp)} features from {os.path.basename(path)}\")\n",
    "\n",
    "    if len(all_images) < 2:\n",
    "        raise ValueError(\"Need at least 2 valid images\")\n",
    "\n",
    "    # Initialize first camera\n",
    "    first_pose = np.hstack((np.eye(3), np.zeros((3, 1))))\n",
    "    camera_poses.append(first_pose)\n",
    "    camera_matrices.append(K @ first_pose)\n",
    "\n",
    "    # Sequential reconstruction\n",
    "    for i in range(1, len(all_images)):\n",
    "        best_pose = None\n",
    "        best_points = None\n",
    "        best_colors = None\n",
    "        best_score = -1\n",
    "\n",
    "        # Try matching with previous views, prioritizing recent ones\n",
    "        prev_indices = list(range(max(0, i-3), i))[::-1]  # Reverse order to try recent first\n",
    "        \n",
    "        for j in prev_indices:\n",
    "            if j >= len(camera_poses):  # Skip if we don't have the camera pose\n",
    "                continue\n",
    "\n",
    "            pose, pts3d, colors, score = process_image_pair(\n",
    "                j, i, all_images, all_descriptors, all_keypoints,\n",
    "                K, camera_poses, camera_matrices\n",
    "            )\n",
    "\n",
    "            if pose is not None and score > best_score:\n",
    "                best_pose = pose\n",
    "                best_points = pts3d\n",
    "                best_colors = colors\n",
    "                best_score = score\n",
    "\n",
    "                # If we got a good result from the most recent frame, stop searching\n",
    "                if j == i-1 and score > 100:  # Threshold for a 'good enough' result\n",
    "                    break\n",
    "\n",
    "        if best_pose is not None:\n",
    "            camera_poses.append(best_pose)\n",
    "            camera_matrices.append(K @ best_pose)\n",
    "            if best_points is not None and best_colors is not None:\n",
    "                points_3d.extend(best_points)\n",
    "                point_colors.extend(best_colors)\n",
    "                print(f\"Added image {i} with {len(best_points)} points (score: {best_score:.2f})\")\n",
    "        else:\n",
    "            print(f\"Warning: Could not find good pose for image {i}\")\n",
    "            # Add dummy pose to maintain indexing\n",
    "            dummy_pose = camera_poses[-1].copy()  # Copy last pose\n",
    "            camera_poses.append(dummy_pose)\n",
    "            camera_matrices.append(K @ dummy_pose)\n",
    "\n",
    "    if len(points_3d) == 0 or len(point_colors) == 0:\n",
    "        raise ValueError(\"Failed to reconstruct any 3D points\")\n",
    "\n",
    "    return np.array(points_3d), np.array(point_colors), camera_poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform multi-view reconstruction\n",
    "mv_points, mv_colors, mv_cameras = reconstruct_multiview(multi_image_paths, K)\n",
    "\n",
    "# Create point clouds for comparison\n",
    "pcd_two_view = o3d.geometry.PointCloud()\n",
    "pcd_two_view.points = o3d.utility.Vector3dVector(point_cloud)\n",
    "pcd_two_view.colors = o3d.utility.Vector3dVector(point_colors)\n",
    "\n",
    "pcd_multi_view = o3d.geometry.PointCloud()\n",
    "pcd_multi_view.points = o3d.utility.Vector3dVector(mv_points)\n",
    "pcd_multi_view.colors = o3d.utility.Vector3dVector(mv_colors)\n",
    "\n",
    "# Clean both point clouds\n",
    "pcd_two_view, _ = pcd_two_view.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)\n",
    "pcd_multi_view, _ = pcd_multi_view.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)\n",
    "\n",
    "print(f\"Two-view reconstruction: {len(pcd_two_view.points)} points\")\n",
    "print(f\"Multi-view reconstruction: {len(pcd_multi_view.points)} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Two-View vs Multi-View Reconstruction\n",
    "\n",
    "Below we visualize both reconstructions side by side. The multi-view reconstruction typically provides:\n",
    "- More complete coverage of the scene\n",
    "- Better handling of occlusions\n",
    "- More accurate point positions due to multiple observations\n",
    "- Higher point density in well-observed regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize two-view reconstruction\n",
    "print(\"Two-view reconstruction:\")\n",
    "o3d.visualization.draw_geometries([pcd_two_view])\n",
    "\n",
    "# Visualize multi-view reconstruction\n",
    "print(\"\\nMulti-view reconstruction:\")\n",
    "o3d.visualization.draw_geometries([pcd_multi_view])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "two_view.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
