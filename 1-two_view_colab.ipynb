{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "# Two-View 3D Reconstruction\n",
    "\n",
    "This notebook implements a simple two-view Structure from Motion approach to create a 3D reconstruction from two images (01.jpg and 02.jpg).\n",
    "\n",
    "## Setup and Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory Introduction\n",
    "\n",
    "Structure from Motion (SfM) is a photogrammetric range imaging technique for estimating three-dimensional structures from two-dimensional image sequences. It works by:\n",
    "\n",
    "1. Finding correspondences between images\n",
    "2. Recovering camera poses (position and orientation)\n",
    "3. Triangulating points to create a 3D reconstruction\n",
    "\n",
    "This notebook demonstrates the classic two-view reconstruction which forms the foundation of more complex multi-view reconstruction systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "  if os.path.exists('/content/Prague_ml_data'):\n",
    "    print(\"Prague_ml_data already exists\")\n",
    "  else:\n",
    "    ! git clone https://github.com/VarunBurde/Prague_ml_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "\n",
    "The code above checks if we're running in Google Colab and clones the necessary data repository if needed. This ensures we have access to the sample images and calibration data regardless of where this notebook is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97438a18",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install open3d networkx matplotlib tqdm opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Libraries\n",
    "\n",
    "- **Open3D**: A library for 3D data processing and visualization\n",
    "- **NetworkX**: A library for network analysis (used for some graph-based operations)\n",
    "- **Matplotlib**: For 2D plotting and visualization\n",
    "- **tqdm**: For progress bars\n",
    "- **OpenCV**: Core library for computer vision algorithms, including feature detection and matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d0d7a7",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import open3d as o3d\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from glob import glob\n",
    "import plotly.graph_objects as go\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For displaying Open3D visualizations in notebooks\n",
    "from google.colab.patches import cv2_imshow\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "def display_open3d_to_notebook(vis, width=900, height=600):\n",
    "    vis.update_renderer()\n",
    "    image = vis.capture_screen_float_buffer()\n",
    "    image_array = np.asarray(image)\n",
    "    image_array = (image_array * 255).astype(np.uint8)\n",
    "    image_array = cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR)\n",
    "    _, encoded_img = cv2.imencode('.png', image_array)\n",
    "    encoded_img = base64.b64encode(encoded_img)\n",
    "    html = f'<img src=\"data:image/png;base64,{encoded_img.decode()}\" width=\"{width}\" height=\"{height}\"/>'\n",
    "    return HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions for Visualization\n",
    "\n",
    "The `display_open3d_to_notebook` function enables visualization of Open3D renderings in the notebook environment. It works by:\n",
    "\n",
    "1. Capturing the rendered Open3D visualization as an image buffer\n",
    "2. Converting the buffer to a numpy array and applying necessary color transformations\n",
    "3. Encoding the image as base64 and embedding it in HTML for display in the notebook\n",
    "\n",
    "This is particularly useful for interactive 3D visualization in environments like Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48944a78",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "First, let's load our two specific images: 01.jpg and 02.jpg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Data for Structure from Motion\n",
    "\n",
    "For two-view reconstruction, we need:\n",
    "\n",
    "1. Two images of the same scene from different viewpoints\n",
    "2. Camera calibration information (intrinsic matrix K)\n",
    "\n",
    "The intrinsic matrix K contains information about the camera's internal parameters:\n",
    "- Focal length (fx, fy)\n",
    "- Principal point (cx, cy)\n",
    "- Skew coefficient\n",
    "\n",
    "This information is essential for accurate 3D reconstruction as it allows us to convert between pixel coordinates and normalized camera coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84126560",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# For Colab: Upload images\n",
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "# Set the paths for our specific images\n",
    "if IN_COLAB:\n",
    "  img_path = \"/content/Prague_ml_data/dataset/two_view_data/images\"\n",
    "  image_paths = [os.path.join(img_path, \"01.jpg\"), os.path.join(img_path, \"02.jpg\")]\n",
    "  K = np.loadtxt(\"/content/Prague_ml_data/dataset/two_view_data/K.txt\")\n",
    "else:\n",
    "  img_path = \"dataset/two_view_data/images\"\n",
    "  image_paths = [os.path.join(img_path, \"01.jpg\"), os.path.join(img_path, \"02.jpg\")]\n",
    "  K = np.loadtxt(\"dataset/two_view_data/K.txt\")\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "for i, path in enumerate(image_paths):\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    img = cv2.imread(path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.title(f\"Image {i+1}: {os.path.basename(path)}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85c2077",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 2. Load Images and Extract Features\n",
    "\n",
    "Now we'll load our two images and extract feature points using SIFT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Detection Theory\n",
    "\n",
    "Feature detection is a crucial first step in 3D reconstruction. We need to identify distinctive points in each image that can be reliably matched between views.\n",
    "\n",
    "**SIFT (Scale-Invariant Feature Transform)**:\n",
    "\n",
    "SIFT is a robust feature detection algorithm that identifies keypoints which are invariant to:\n",
    "- Scale changes\n",
    "- Rotation\n",
    "- Illumination changes\n",
    "- Viewpoint changes (to some extent)\n",
    "\n",
    "The SIFT algorithm works by:\n",
    "1. Building a scale-space pyramid of the image\n",
    "2. Finding extrema (maxima/minima) in the difference of Gaussian images\n",
    "3. Refining keypoint locations\n",
    "4. Eliminating low-contrast and edge keypoints\n",
    "5. Assigning orientation to each keypoint\n",
    "6. Creating descriptors (128-dimensional vectors) for each keypoint\n",
    "\n",
    "These descriptors capture the local appearance of the image around each keypoint and are used for matching between images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e8006d",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Load our two images\n",
    "images = []\n",
    "images_rgb = []\n",
    "\n",
    "for path in image_paths:\n",
    "    img = cv2.imread(path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    images.append(img)\n",
    "    images_rgb.append(img_rgb)\n",
    "\n",
    "print(f\"Loaded {len(images)} images\")\n",
    "\n",
    "# Create SIFT detector with more aggressive parameters for denser features\n",
    "sift = cv2.SIFT_create(\n",
    "    nfeatures=0,  # No limit on number of features\n",
    "    contrastThreshold=0.04,  # Lower threshold for more features\n",
    "    edgeThreshold=10,  # Higher threshold for more features\n",
    "    sigma=1.6  # Default sigma\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIFT Parameter Tuning\n",
    "\n",
    "The SIFT detector is created with specific parameters to optimize feature detection:\n",
    "\n",
    "- **nfeatures=0**: No limit on the number of features to detect (default is 0, which means no limit)\n",
    "- **contrastThreshold=0.04**: Lower values detect more features but may include less distinctive ones\n",
    "- **edgeThreshold=10**: Higher values allow more features near edges\n",
    "- **sigma=1.6**: Initial Gaussian blur applied to the image\n",
    "\n",
    "Tuning these parameters involves balancing quantity (more features) with quality (more distinctive features). For 3D reconstruction, we generally want a dense set of reliable features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd0154a",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Detect features in both images\n",
    "all_kp = []\n",
    "all_des = []\n",
    "\n",
    "for i, img_rgb in enumerate(images_rgb):\n",
    "    img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\n",
    "    img_eq = cv2.equalizeHist(img_gray)\n",
    "    \n",
    "    # Detect features at multiple scales\n",
    "    kp_standard, des_standard = sift.detectAndCompute(img_rgb, None)\n",
    "    kp_eq, des_eq = sift.detectAndCompute(img_eq, None)\n",
    "    \n",
    "    # Combine features\n",
    "    if des_standard is not None and des_eq is not None:\n",
    "        kp = kp_standard + kp_eq\n",
    "        des = np.vstack((des_standard, des_eq))\n",
    "    elif des_standard is not None:\n",
    "        kp = kp_standard\n",
    "        des = des_standard\n",
    "    elif des_eq is not None:\n",
    "        kp = kp_eq\n",
    "        des = des_eq\n",
    "    else:\n",
    "        kp = []\n",
    "        des = np.array([])\n",
    "    \n",
    "    all_kp.append(kp)\n",
    "    all_des.append(des)\n",
    "    print(f\"Image {i+1} ({os.path.basename(image_paths[i])}): Detected {len(kp)} keypoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enhanced Feature Detection\n",
    "\n",
    "The code above implements a robust feature detection approach by:\n",
    "\n",
    "1. **Multi-scale processing**: Detecting features in both RGB and histogram-equalized grayscale versions of the image\n",
    "\n",
    "2. **Histogram equalization**: This technique improves contrast in the image, which can reveal features in darker or low-contrast regions\n",
    "\n",
    "3. **Feature combination**: Combining features from both versions increases the overall feature density\n",
    "\n",
    "This approach helps ensure we have a rich set of features throughout the image, which is essential for a complete 3D reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53c2af1",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "### Visualize Detected Features\n",
    "\n",
    "Let's visualize the keypoints detected in both images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0eca61",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Display keypoints on both images\n",
    "plt.figure(figsize=(15, 7))\n",
    "for i in range(2):\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    img_with_kp = cv2.drawKeypoints(images_rgb[i], all_kp[i], None, \n",
    "                                    flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    plt.imshow(img_with_kp)\n",
    "    plt.title(f\"Image {i+1} ({os.path.basename(image_paths[i])}): {len(all_kp[i])} keypoints\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Visualization\n",
    "\n",
    "The visualization shows the detected SIFT keypoints on each image. Each keypoint is represented by a circle:\n",
    "- The center indicates the keypoint location\n",
    "- The radius indicates the keypoint scale\n",
    "- The line indicates the keypoint orientation\n",
    "\n",
    "A good feature distribution should cover the image uniformly, with concentration in areas with distinctive texture or structures. The size of the circles indicates the scale at which the feature was detected - larger circles represent features detected at coarser scales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5021c53f",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 3. Match Features Between Images\n",
    "\n",
    "Now we'll match features between our two images to find correspondences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Matching Theory\n",
    "\n",
    "Feature matching is the process of finding correspondences between keypoints in different images. This is a critical step in 3D reconstruction as these correspondences will be used to establish geometric relationships between views.\n",
    "\n",
    "**FLANN-based Matching**:\n",
    "\n",
    "Fast Library for Approximate Nearest Neighbors (FLANN) is an efficient algorithm for finding approximate nearest neighbors in high-dimensional spaces. For feature matching:\n",
    "\n",
    "1. **KD-Tree Algorithm**: Organizes descriptors in a tree-like structure for efficient searching\n",
    "\n",
    "2. **K-Nearest Neighbor (KNN) Matching**: For each keypoint in the first image, find the two closest matches in the second image\n",
    "\n",
    "3. **Lowe's Ratio Test**: Filter matches by comparing the distances between the best and second-best match\n",
    "   - If best_match_distance < ratio * second_best_match_distance, keep the match\n",
    "   - This helps ensure that matches are distinctive and reduces false positives\n",
    "\n",
    "4. **Cross-Checking**: An additional validation step that ensures matches are consistent in both directions\n",
    "   - A match is valid only if keypoint A in image 1 matches to keypoint B in image 2\n",
    "   - AND keypoint B in image 2 matches back to keypoint A in image 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a967322",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Improved FLANN matcher setup\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "search_params = dict(checks=100)  # More checks for better accuracy\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "# Function to perform feature matching with cross-check\n",
    "def match_features(des1, des2, lowes_ratio=0.75, cross_check=True):\n",
    "    # Forward matching (img1 -> img2)\n",
    "    if des1 is None or des2 is None or len(des1) == 0 or len(des2) == 0:\n",
    "        return []\n",
    "    \n",
    "    matches12 = flann.knnMatch(des1, des2, k=2)\n",
    "    good_matches12 = []\n",
    "    for m, n in matches12:\n",
    "        if m.distance < lowes_ratio * n.distance:\n",
    "            good_matches12.append(m)\n",
    "    \n",
    "    if not cross_check:\n",
    "        return good_matches12\n",
    "    \n",
    "    # Backward matching (img2 -> img1) for cross-check\n",
    "    matches21 = flann.knnMatch(des2, des1, k=2)\n",
    "    good_matches21 = []\n",
    "    for m, n in matches21:\n",
    "        if m.distance < lowes_ratio * n.distance:\n",
    "            good_matches21.append(m)\n",
    "    \n",
    "    # Cross-check: keep matches that are consistent in both directions\n",
    "    cross_matches = []\n",
    "    for match12 in good_matches12:\n",
    "        for match21 in good_matches21:\n",
    "            # Check if match is consistent in both directions\n",
    "            if match12.queryIdx == match21.trainIdx and match12.trainIdx == match21.queryIdx:\n",
    "                cross_matches.append(match12)\n",
    "                break\n",
    "    \n",
    "    return cross_matches\n",
    "\n",
    "# Get matched points from keypoints and matches\n",
    "def get_matched_points(kp1, kp2, matches):\n",
    "    pts1 = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    pts2 = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    return pts1, pts2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust Feature Matching Implementation\n",
    "\n",
    "The code implements a robust feature matching approach with several key components:\n",
    "\n",
    "1. **FLANN Matcher Configuration**:\n",
    "   - Uses KD-Tree algorithm with 5 trees for efficient searching\n",
    "   - Sets checks=100 to increase matching accuracy (more computation but better results)\n",
    "\n",
    "2. **Lowe's Ratio Test**:\n",
    "   - The default ratio is 0.75 (a common value in practice)\n",
    "   - Lower values make matching more strict, higher values allow more matches but may include more false positives\n",
    "\n",
    "3. **Cross-Check Validation**:\n",
    "   - Performs matching in both directions (image1→image2 and image2→image1)\n",
    "   - Only keeps matches that are mutually consistent\n",
    "   - This significantly improves match quality at the cost of reducing match quantity\n",
    "\n",
    "4. **Helper Function for Extracting Point Coordinates**:\n",
    "   - Converts OpenCV match objects into actual point coordinates\n",
    "   - The resulting coordinates will be used for geometric calculations in the next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match features between our two images (01.jpg and 02.jpg)\n",
    "idx1, idx2 = 0, 1  # Using image indices 0 and 1 (01.jpg and 02.jpg)\n",
    "good_matches = match_features(all_des[idx1], all_des[idx2], lowes_ratio=0.75)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Feature Matching\n",
    "\n",
    "Here we apply the feature matching between our two images using the robust matching function defined earlier. The function:\n",
    "\n",
    "1. Takes feature descriptors from both images\n",
    "2. Applies FLANN-based KNN matching in both directions\n",
    "3. Filters matches using Lowe's ratio test with a threshold of 0.75\n",
    "4. Applies cross-checking to ensure matches are consistent\n",
    "\n",
    "The result is a set of high-quality matches that will form the basis for our geometric calculations in the following steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Found {len(good_matches)} good matches between {os.path.basename(image_paths[0])} and {os.path.basename(image_paths[1])}\")\n",
    "\n",
    "\n",
    "img1 = images_rgb[idx1]\n",
    "img2 = images_rgb[idx2]\n",
    "h1, w1 = img1.shape[:2]\n",
    "h2, w2 = img2.shape[:2]\n",
    "matched_img = np.zeros((max(h1, h2), w1 + w2, 3), dtype=np.uint8)\n",
    "matched_img[:h1, :w1] = img1\n",
    "matched_img[:h2, w1:w1 + w2] = img2\n",
    "\n",
    "# Draw keypoints\n",
    "for m in good_matches[:10]:\n",
    "    pt1 = tuple(map(int, all_kp[idx1][m.queryIdx].pt))\n",
    "    pt2 = tuple(map(int, all_kp[idx2][m.trainIdx].pt))\n",
    "    pt2 = (int(pt2[0] + w1), int(pt2[1]))  # shift pt2 x-coord\n",
    "\n",
    "    # Draw circles\n",
    "    cv2.circle(matched_img, pt1, 20, (0, 255, 0), -1)  # radius 6, green\n",
    "    cv2.circle(matched_img, pt2, 20, (0, 255, 0), -1)\n",
    "\n",
    "    # Draw connecting line\n",
    "    cv2.line(matched_img, pt1, pt2, (255, 0, 0),10)  # blue line, thickness 2\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(matched_img)\n",
    "plt.title(f'Matches between {os.path.basename(image_paths[0])} and {os.path.basename(image_paths[1])}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match Visualization\n",
    "\n",
    "The visualization shows the top 10 feature matches between the two images. Good matches should have the following characteristics:\n",
    "\n",
    "1. **Consistent Motion**: The connecting lines should generally follow similar directions (indicating consistent camera motion)\n",
    "\n",
    "2. **Spatial Distribution**: Matches should be distributed across the images (not concentrated in one area)\n",
    "\n",
    "3. **Low Outlier Rate**: There should be few obvious mismatches (lines connecting unrelated points)\n",
    "\n",
    "This visual inspection helps confirm the quality of feature matching before proceeding to the geometric calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40bc56b",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 4. Two-View Reconstruction\n",
    "\n",
    "Now we'll use the matched features to perform 3D reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epipolar Geometry Theory\n",
    "\n",
    "Two-view reconstruction is based on epipolar geometry, which describes the geometric relationship between two camera views:\n",
    "\n",
    "**Key Concepts**:\n",
    "\n",
    "1. **Essential Matrix (E)**: Encodes the geometric relationship between two calibrated cameras\n",
    "   - E = [t]₊R (where [t]₊ is the skew-symmetric matrix of the translation vector, and R is the rotation matrix)\n",
    "   - For corresponding points x and x', we have: x'ᵀEx = 0 (epipolar constraint)\n",
    "\n",
    "2. **Camera Pose Recovery**:\n",
    "   - From the essential matrix, we can extract the relative rotation (R) and translation (t) between cameras\n",
    "   - By convention, the first camera is at the origin (identity rotation, zero translation)\n",
    "   - The second camera's pose is defined relative to the first\n",
    "\n",
    "3. **Triangulation**:\n",
    "   - Once we know both camera poses and have matched points, we can triangulate 3D points\n",
    "   - Each pair of matching 2D points defines rays in 3D space\n",
    "   - The intersection of these rays (or closest point to both rays) gives us the 3D point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613ab442",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Initialize camera poses (R|t) for each image\n",
    "# First camera (01.jpg) is at origin with identity rotation\n",
    "camera_poses = [np.hstack((np.eye(3), np.zeros((3, 1))))]\n",
    "camera_matrices = [K @ camera_poses[0]]\n",
    "\n",
    "# Structure to store 3D points and their 2D observations\n",
    "point_cloud = []\n",
    "point_colors = []\n",
    "\n",
    "pts1, pts2 = get_matched_points(all_kp[idx1], all_kp[idx2], good_matches)\n",
    "\n",
    "# Calculate essential matrix with robust parameters\n",
    "E, mask = cv2.findEssentialMat(pts1, pts2, K, method=cv2.RANSAC, prob=0.999, threshold=2.0)\n",
    "\n",
    "# Recover pose for second camera (02.jpg)\n",
    "_, R, t, mask = cv2.recoverPose(E, pts1, pts2, K, mask=mask)\n",
    "\n",
    "# Set second camera pose\n",
    "camera_poses.append(np.hstack((R, t)))\n",
    "camera_matrices.append(K @ camera_poses[1])\n",
    "\n",
    "print(f\"Camera 1 (01.jpg) pose:\\n{camera_poses[0]}\")\n",
    "print(f\"\\nCamera 2 (02.jpg) pose:\\n{camera_poses[1]}\")\n",
    "\n",
    "# Filter points using mask\n",
    "mask = mask.ravel() == 1\n",
    "pts1_good = pts1[mask]\n",
    "pts2_good = pts2[mask]\n",
    "\n",
    "print(f\"\\nUsing {np.sum(mask)} inlier matches for triangulation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camera Pose Estimation\n",
    "\n",
    "This section implements the first major step in 3D reconstruction: estimating the relative poses of the two cameras. The process includes:\n",
    "\n",
    "1. **Camera Initialization**:\n",
    "   - The first camera (01.jpg) is defined as the world origin with identity rotation and zero translation\n",
    "   - This establishes our coordinate system reference frame\n",
    "\n",
    "2. **Essential Matrix Estimation**:\n",
    "   - Uses `cv2.findEssentialMat` with RANSAC to robustly estimate the essential matrix\n",
    "   - The RANSAC method helps eliminate outlier correspondences\n",
    "   - Parameters include:\n",
    "     - `prob=0.999`: High probability of finding the correct model\n",
    "     - `threshold=2.0`: Error threshold for inlier classification (in pixels)\n",
    "\n",
    "3. **Pose Recovery**:\n",
    "   - Uses `cv2.recoverPose` to extract rotation (R) and translation (t) from the essential matrix\n",
    "   - This gives us the relative pose of the second camera\n",
    "   - The function also returns a refined mask indicating which points are geometrically consistent\n",
    "\n",
    "4. **Point Filtering**:\n",
    "   - Only points that are inliers to the essential matrix are used for triangulation\n",
    "   - This further improves reconstruction quality by eliminating outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33257416",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Triangulate points\n",
    "pts1_good = pts1_good.reshape(-1, 2).T\n",
    "pts2_good = pts2_good.reshape(-1, 2).T\n",
    "points_4D = cv2.triangulatePoints(camera_matrices[0], camera_matrices[1], pts1_good, pts2_good)\n",
    "points_3D = points_4D / points_4D[3]  # Convert to Cartesian\n",
    "points_3D = points_3D[:3, :].T\n",
    "\n",
    "# Filter points by depth\n",
    "valid_points = []\n",
    "valid_colors = []\n",
    "\n",
    "for i in range(points_3D.shape[0]):\n",
    "    point = points_3D[i]\n",
    "    # Check if point is in front of both cameras\n",
    "    if point[2] > 0:\n",
    "        # Get corresponding 2D points\n",
    "        x1, y1 = pts1_good[:, i]\n",
    "        \n",
    "        # Extract color from first image\n",
    "        x1_int, y1_int = int(round(x1)), int(round(y1))\n",
    "        if 0 <= x1_int < images_rgb[idx1].shape[1] and 0 <= y1_int < images_rgb[idx1].shape[0]:\n",
    "            color = images_rgb[idx1][y1_int, x1_int] / 255.0\n",
    "            valid_points.append(point)\n",
    "            valid_colors.append(color)\n",
    "\n",
    "point_cloud = np.array(valid_points)\n",
    "point_colors = np.array(valid_colors)\n",
    "\n",
    "print(f\"Reconstructed point cloud has {len(point_cloud)} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D Triangulation\n",
    "\n",
    "Triangulation is the process of determining the 3D position of points from their 2D projections in multiple images. This code implements triangulation with the following steps:\n",
    "\n",
    "1. **Triangulation Algorithm**:\n",
    "   - `cv2.triangulatePoints` computes 3D points in homogeneous coordinates (4D)\n",
    "   - It uses Direct Linear Transform (DLT) algorithm to solve the triangulation problem\n",
    "   - Inputs are the camera projection matrices and corresponding 2D points\n",
    "\n",
    "2. **Homogeneous to Cartesian Conversion**:\n",
    "   - Divides the 4D homogeneous coordinates by the fourth component (w)\n",
    "   - Takes the first three components (x, y, z) as the Cartesian coordinates\n",
    "\n",
    "3. **Point Filtering**:\n",
    "   - Cheirality check: Only keeps points with positive z-coordinate (in front of both cameras)\n",
    "   - This eliminates points that are behind either camera (physically impossible)\n",
    "\n",
    "4. **Color Assignment**:\n",
    "   - Assigns colors to 3D points based on their color in the first image\n",
    "   - This allows for a visually meaningful 3D reconstruction\n",
    "\n",
    "The resulting point cloud represents the 3D structure of the scene visible in both images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a21ecfe",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "### Visualize Point Cloud\n",
    "Let's visualize the 3D point cloud reconstructed from our two images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb564960",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Create Open3D point cloud for visualization\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(point_cloud)\n",
    "pcd.colors = o3d.utility.Vector3dVector(point_colors)\n",
    "\n",
    "# Remove outliers for cleaner visualization\n",
    "pcd, _ = pcd.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point Cloud Refinement\n",
    "\n",
    "The final point cloud is prepared for visualization with some additional processing:\n",
    "\n",
    "1. **Open3D Point Cloud Creation**:\n",
    "   - Converts numpy arrays to Open3D's Vector3dVector format\n",
    "   - Assigns both 3D positions and colors to the point cloud\n",
    "\n",
    "2. **Statistical Outlier Removal**:\n",
    "   - Uses Open3D's statistical outlier removal algorithm to clean the point cloud\n",
    "   - Parameters:\n",
    "     - `nb_neighbors=20`: Uses 20 nearest neighbors to evaluate each point\n",
    "     - `std_ratio=2.0`: Points with average distance larger than 2 standard deviations are removed\n",
    "   - This helps remove isolated points that are likely reconstruction errors\n",
    "\n",
    "A clean point cloud is important for both visualization quality and subsequent processing steps if the reconstruction were to be further refined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_geometries(geometries):\n",
    "    graph_objects = []\n",
    "\n",
    "    for geometry in geometries:\n",
    "        geometry_type = geometry.get_geometry_type()\n",
    "\n",
    "        if geometry_type == o3d.geometry.Geometry.Type.PointCloud:\n",
    "            points = np.asarray(geometry.points)\n",
    "            colors = None\n",
    "            if geometry.has_colors():\n",
    "                colors = np.asarray(geometry.colors)\n",
    "            elif geometry.has_normals():\n",
    "                colors = (0.5, 0.5, 0.5) + np.asarray(geometry.normals) * 0.5\n",
    "            else:\n",
    "                geometry.paint_uniform_color((1.0, 0.0, 0.0))\n",
    "                colors = np.asarray(geometry.colors)\n",
    "\n",
    "            scatter_3d = go.Scatter3d(x=points[:,0], y=points[:,1], z=points[:,2], mode='markers', marker=dict(size=1, color=colors))\n",
    "            graph_objects.append(scatter_3d)\n",
    "\n",
    "        if geometry_type == o3d.geometry.Geometry.Type.TriangleMesh:\n",
    "            triangles = np.asarray(geometry.triangles)\n",
    "            vertices = np.asarray(geometry.vertices)\n",
    "            colors = None\n",
    "            if geometry.has_triangle_normals():\n",
    "                colors = (0.5, 0.5, 0.5) + np.asarray(geometry.triangle_normals) * 0.5\n",
    "                colors = tuple(map(tuple, colors))\n",
    "            else:\n",
    "                colors = (1.0, 0.0, 0.0)\n",
    "\n",
    "            mesh_3d = go.Mesh3d(x=vertices[:,0], y=vertices[:,1], z=vertices[:,2], i=triangles[:,0], j=triangles[:,1], k=triangles[:,2], facecolor=colors, opacity=0.50)\n",
    "            graph_objects.append(mesh_3d)\n",
    "\n",
    "    fig = go.Figure(\n",
    "        data=graph_objects,\n",
    "        layout=dict(\n",
    "            scene=dict(\n",
    "                xaxis=dict(visible=False),\n",
    "                yaxis=dict(visible=False),\n",
    "                zaxis=dict(visible=False)\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive 3D Visualization\n",
    "\n",
    "This helper function enables interactive 3D visualization of the point cloud using Plotly:\n",
    "\n",
    "1. **Geometry Processing**:\n",
    "   - Handles different types of Open3D geometries (PointCloud, TriangleMesh)\n",
    "   - Extracts points, vertices, triangles, and colors from the geometry objects\n",
    "\n",
    "2. **Plotly Integration**:\n",
    "   - Creates appropriate Plotly 3D visualization objects (Scatter3d for points, Mesh3d for meshes)\n",
    "   - Configures the layout for clean visualization without axis clutter\n",
    "\n",
    "3. **Visualization Features**:\n",
    "   - Points are rendered as small markers with their original colors\n",
    "   - Meshes would be rendered with semi-transparency for better visualization\n",
    "   - The resulting visualization is interactive, allowing rotation, zoom, and pan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries = draw_geometries # replace function\n",
    "o3d.visualization.draw_geometries([pcd])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final 3D Point Cloud Visualization\n",
    "\n",
    "The final visualization displays the reconstructed 3D point cloud in an interactive viewer. This allows for:\n",
    "\n",
    "1. **Inspection of Reconstruction Quality**:\n",
    "   - Examining the density and distribution of reconstructed points\n",
    "   - Checking for structural coherence and recognizable scene elements\n",
    "\n",
    "2. **Visualization Benefits**:\n",
    "   - Interactive rotation helps understand the true 3D nature of the reconstruction\n",
    "   - Color information aids in identifying scene components\n",
    "\n",
    "3. **Potential Next Steps**:\n",
    "   - This point cloud could be further processed for mesh reconstruction\n",
    "   - Additional views could be incorporated for a denser, more complete reconstruction\n",
    "   - Bundle adjustment could refine camera poses and 3D point positions\n",
    "\n",
    "The two-view reconstruction demonstrates the fundamental principles of Structure from Motion, which form the basis for more complex multi-view reconstruction systems used in applications like 3D scanning, AR/VR content creation, and robotics."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "two_view.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
