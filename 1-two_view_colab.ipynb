{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "# Two-View 3D Reconstruction\n",
    "\n",
    "This notebook implements a simple two-view Structure from Motion approach to create a 3D reconstruction from two images (01.jpg and 02.jpg).\n",
    "\n",
    "## Setup and Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "  if os.path.exists('/content/Prague_ml_data'):\n",
    "    print(\"Prague_ml_data already exists\")\n",
    "  else:\n",
    "    ! git clone https://github.com/VarunBurde/Prague_ml_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97438a18",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install open3d networkx matplotlib tqdm opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d0d7a7",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import open3d as o3d\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from glob import glob\n",
    "import plotly.graph_objects as go\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For displaying Open3D visualizations in notebooks\n",
    "from google.colab.patches import cv2_imshow\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "def display_open3d_to_notebook(vis, width=900, height=600):\n",
    "    vis.update_renderer()\n",
    "    image = vis.capture_screen_float_buffer()\n",
    "    image_array = np.asarray(image)\n",
    "    image_array = (image_array * 255).astype(np.uint8)\n",
    "    image_array = cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR)\n",
    "    _, encoded_img = cv2.imencode('.png', image_array)\n",
    "    encoded_img = base64.b64encode(encoded_img)\n",
    "    html = f'<img src=\"data:image/png;base64,{encoded_img.decode()}\" width=\"{width}\" height=\"{height}\"/>'\n",
    "    return HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48944a78",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "First, let's load our two specific images: 01.jpg and 02.jpg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84126560",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# For Colab: Upload images\n",
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "# Set the paths for our specific images\n",
    "if IN_COLAB:\n",
    "  img_path = \"/content/Prague_ml_data/dataset/two_view_data/images\"\n",
    "  image_paths = [os.path.join(img_path, \"01.jpg\"), os.path.join(img_path, \"02.jpg\")]\n",
    "  K = np.loadtxt(\"/content/Prague_ml_data/dataset/two_view_data/K.txt\")\n",
    "else:\n",
    "  img_path = \"dataset/two_view_data/images\"\n",
    "  image_paths = [os.path.join(img_path, \"01.jpg\"), os.path.join(img_path, \"02.jpg\")]\n",
    "  K = np.loadtxt(\"dataset/two_view_data/K.txt\")\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "for i, path in enumerate(image_paths):\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    img = cv2.imread(path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.title(f\"Image {i+1}: {os.path.basename(path)}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85c2077",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 2. Load Images and Extract Features\n",
    "\n",
    "Now we'll load our two images and extract feature points using SIFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e8006d",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Load our two images\n",
    "images = []\n",
    "images_rgb = []\n",
    "\n",
    "for path in image_paths:\n",
    "    img = cv2.imread(path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    images.append(img)\n",
    "    images_rgb.append(img_rgb)\n",
    "\n",
    "print(f\"Loaded {len(images)} images\")\n",
    "\n",
    "# Create SIFT detector with more aggressive parameters for denser features\n",
    "sift = cv2.SIFT_create(\n",
    "    nfeatures=0,  # No limit on number of features\n",
    "    contrastThreshold=0.04,  # Lower threshold for more features\n",
    "    edgeThreshold=10,  # Higher threshold for more features\n",
    "    sigma=1.6  # Default sigma\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd0154a",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Detect features in both images\n",
    "all_kp = []\n",
    "all_des = []\n",
    "\n",
    "for i, img_rgb in enumerate(images_rgb):\n",
    "    img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\n",
    "    img_eq = cv2.equalizeHist(img_gray)\n",
    "    \n",
    "    # Detect features at multiple scales\n",
    "    kp_standard, des_standard = sift.detectAndCompute(img_rgb, None)\n",
    "    kp_eq, des_eq = sift.detectAndCompute(img_eq, None)\n",
    "    \n",
    "    # Combine features\n",
    "    if des_standard is not None and des_eq is not None:\n",
    "        kp = kp_standard + kp_eq\n",
    "        des = np.vstack((des_standard, des_eq))\n",
    "    elif des_standard is not None:\n",
    "        kp = kp_standard\n",
    "        des = des_standard\n",
    "    elif des_eq is not None:\n",
    "        kp = kp_eq\n",
    "        des = des_eq\n",
    "    else:\n",
    "        kp = []\n",
    "        des = np.array([])\n",
    "    \n",
    "    all_kp.append(kp)\n",
    "    all_des.append(des)\n",
    "    print(f\"Image {i+1} ({os.path.basename(image_paths[i])}): Detected {len(kp)} keypoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53c2af1",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "### Visualize Detected Features\n",
    "\n",
    "Let's visualize the keypoints detected in both images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0eca61",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Display keypoints on both images\n",
    "plt.figure(figsize=(15, 7))\n",
    "for i in range(2):\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    img_with_kp = cv2.drawKeypoints(images_rgb[i], all_kp[i], None, \n",
    "                                    flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    plt.imshow(img_with_kp)\n",
    "    plt.title(f\"Image {i+1} ({os.path.basename(image_paths[i])}): {len(all_kp[i])} keypoints\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5021c53f",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 3. Match Features Between Images\n",
    "\n",
    "Now we'll match features between our two images to find correspondences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a967322",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Improved FLANN matcher setup\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "search_params = dict(checks=100)  # More checks for better accuracy\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "# Function to perform feature matching with cross-check\n",
    "def match_features(des1, des2, lowes_ratio=0.75, cross_check=True):\n",
    "    # Forward matching (img1 -> img2)\n",
    "    if des1 is None or des2 is None or len(des1) == 0 or len(des2) == 0:\n",
    "        return []\n",
    "    \n",
    "    matches12 = flann.knnMatch(des1, des2, k=2)\n",
    "    good_matches12 = []\n",
    "    for m, n in matches12:\n",
    "        if m.distance < lowes_ratio * n.distance:\n",
    "            good_matches12.append(m)\n",
    "    \n",
    "    if not cross_check:\n",
    "        return good_matches12\n",
    "    \n",
    "    # Backward matching (img2 -> img1) for cross-check\n",
    "    matches21 = flann.knnMatch(des2, des1, k=2)\n",
    "    good_matches21 = []\n",
    "    for m, n in matches21:\n",
    "        if m.distance < lowes_ratio * n.distance:\n",
    "            good_matches21.append(m)\n",
    "    \n",
    "    # Cross-check: keep matches that are consistent in both directions\n",
    "    cross_matches = []\n",
    "    for match12 in good_matches12:\n",
    "        for match21 in good_matches21:\n",
    "            # Check if match is consistent in both directions\n",
    "            if match12.queryIdx == match21.trainIdx and match12.trainIdx == match21.queryIdx:\n",
    "                cross_matches.append(match12)\n",
    "                break\n",
    "    \n",
    "    return cross_matches\n",
    "\n",
    "# Get matched points from keypoints and matches\n",
    "def get_matched_points(kp1, kp2, matches):\n",
    "    pts1 = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    pts2 = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    return pts1, pts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match features between our two images (01.jpg and 02.jpg)\n",
    "idx1, idx2 = 0, 1  # Using image indices 0 and 1 (01.jpg and 02.jpg)\n",
    "good_matches = match_features(all_des[idx1], all_des[idx2], lowes_ratio=0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Found {len(good_matches)} good matches between {os.path.basename(image_paths[0])} and {os.path.basename(image_paths[1])}\")\n",
    "\n",
    "# # Display matches for verification\n",
    "# match_img = cv2.drawMatches(\n",
    "#     images_rgb[idx1], all_kp[idx1], \n",
    "#     images_rgb[idx2], all_kp[idx2], \n",
    "#     good_matches[:100], None, \n",
    "#     flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    "# )\n",
    "\n",
    "#or\n",
    "\n",
    "mg1 = images_rgb[idx1]\n",
    "img2 = images_rgb[idx2]\n",
    "h1, w1 = img1.shape[:2]\n",
    "h2, w2 = img2.shape[:2]\n",
    "matched_img = np.zeros((max(h1, h2), w1 + w2, 3), dtype=np.uint8)\n",
    "matched_img[:h1, :w1] = img1\n",
    "matched_img[:h2, w1:w1 + w2] = img2\n",
    "\n",
    "# Draw keypoints\n",
    "for m in good_matches[:10]:\n",
    "    pt1 = tuple(map(int, all_kp[idx1][m.queryIdx].pt))\n",
    "    pt2 = tuple(map(int, all_kp[idx2][m.trainIdx].pt))\n",
    "    pt2 = (int(pt2[0] + w1), int(pt2[1]))  # shift pt2 x-coord\n",
    "\n",
    "    # Draw circles\n",
    "    cv2.circle(matched_img, pt1, 20, (0, 255, 0), -1)  # radius 6, green\n",
    "    cv2.circle(matched_img, pt2, 20, (0, 255, 0), -1)\n",
    "\n",
    "    # Draw connecting line\n",
    "    cv2.line(matched_img, pt1, pt2, (255, 0, 0),10)  # blue line, thickness 2\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(match_img)\n",
    "plt.title(f'Matches between {os.path.basename(image_paths[0])} and {os.path.basename(image_paths[1])}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40bc56b",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 4. Two-View Reconstruction\n",
    "\n",
    "Now we'll use the matched features to perform 3D reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613ab442",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Initialize camera poses (R|t) for each image\n",
    "# First camera (01.jpg) is at origin with identity rotation\n",
    "camera_poses = [np.hstack((np.eye(3), np.zeros((3, 1))))]\n",
    "camera_matrices = [K @ camera_poses[0]]\n",
    "\n",
    "# Structure to store 3D points and their 2D observations\n",
    "point_cloud = []\n",
    "point_colors = []\n",
    "\n",
    "pts1, pts2 = get_matched_points(all_kp[idx1], all_kp[idx2], good_matches)\n",
    "\n",
    "# Calculate essential matrix with robust parameters\n",
    "E, mask = cv2.findEssentialMat(pts1, pts2, K, method=cv2.RANSAC, prob=0.999, threshold=2.0)\n",
    "\n",
    "# Recover pose for second camera (02.jpg)\n",
    "_, R, t, mask = cv2.recoverPose(E, pts1, pts2, K, mask=mask)\n",
    "\n",
    "# Set second camera pose\n",
    "camera_poses.append(np.hstack((R, t)))\n",
    "camera_matrices.append(K @ camera_poses[1])\n",
    "\n",
    "print(f\"Camera 1 (01.jpg) pose:\\n{camera_poses[0]}\")\n",
    "print(f\"\\nCamera 2 (02.jpg) pose:\\n{camera_poses[1]}\")\n",
    "\n",
    "# Filter points using mask\n",
    "mask = mask.ravel() == 1\n",
    "pts1_good = pts1[mask]\n",
    "pts2_good = pts2[mask]\n",
    "\n",
    "print(f\"\\nUsing {np.sum(mask)} inlier matches for triangulation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33257416",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Triangulate points\n",
    "pts1_good = pts1_good.reshape(-1, 2).T\n",
    "pts2_good = pts2_good.reshape(-1, 2).T\n",
    "points_4D = cv2.triangulatePoints(camera_matrices[0], camera_matrices[1], pts1_good, pts2_good)\n",
    "points_3D = points_4D / points_4D[3]  # Convert to Cartesian\n",
    "points_3D = points_3D[:3, :].T\n",
    "\n",
    "# Filter points by depth\n",
    "valid_points = []\n",
    "valid_colors = []\n",
    "\n",
    "for i in range(points_3D.shape[0]):\n",
    "    point = points_3D[i]\n",
    "    # Check if point is in front of both cameras\n",
    "    if point[2] > 0:\n",
    "        # Get corresponding 2D points\n",
    "        x1, y1 = pts1_good[:, i]\n",
    "        \n",
    "        # Extract color from first image\n",
    "        x1_int, y1_int = int(round(x1)), int(round(y1))\n",
    "        if 0 <= x1_int < images_rgb[idx1].shape[1] and 0 <= y1_int < images_rgb[idx1].shape[0]:\n",
    "            color = images_rgb[idx1][y1_int, x1_int] / 255.0\n",
    "            valid_points.append(point)\n",
    "            valid_colors.append(color)\n",
    "\n",
    "point_cloud = np.array(valid_points)\n",
    "point_colors = np.array(valid_colors)\n",
    "\n",
    "print(f\"Reconstructed point cloud has {len(point_cloud)} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a21ecfe",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "### Visualize Point Cloud\n",
    "Let's visualize the 3D point cloud reconstructed from our two images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb564960",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Create Open3D point cloud for visualization\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(point_cloud)\n",
    "pcd.colors = o3d.utility.Vector3dVector(point_colors)\n",
    "\n",
    "# Remove outliers for cleaner visualization\n",
    "pcd, _ = pcd.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_geometries(geometries):\n",
    "    graph_objects = []\n",
    "\n",
    "    for geometry in geometries:\n",
    "        geometry_type = geometry.get_geometry_type()\n",
    "\n",
    "        if geometry_type == o3d.geometry.Geometry.Type.PointCloud:\n",
    "            points = np.asarray(geometry.points)\n",
    "            colors = None\n",
    "            if geometry.has_colors():\n",
    "                colors = np.asarray(geometry.colors)\n",
    "            elif geometry.has_normals():\n",
    "                colors = (0.5, 0.5, 0.5) + np.asarray(geometry.normals) * 0.5\n",
    "            else:\n",
    "                geometry.paint_uniform_color((1.0, 0.0, 0.0))\n",
    "                colors = np.asarray(geometry.colors)\n",
    "\n",
    "            scatter_3d = go.Scatter3d(x=points[:,0], y=points[:,1], z=points[:,2], mode='markers', marker=dict(size=1, color=colors))\n",
    "            graph_objects.append(scatter_3d)\n",
    "\n",
    "        if geometry_type == o3d.geometry.Geometry.Type.TriangleMesh:\n",
    "            triangles = np.asarray(geometry.triangles)\n",
    "            vertices = np.asarray(geometry.vertices)\n",
    "            colors = None\n",
    "            if geometry.has_triangle_normals():\n",
    "                colors = (0.5, 0.5, 0.5) + np.asarray(geometry.triangle_normals) * 0.5\n",
    "                colors = tuple(map(tuple, colors))\n",
    "            else:\n",
    "                colors = (1.0, 0.0, 0.0)\n",
    "\n",
    "            mesh_3d = go.Mesh3d(x=vertices[:,0], y=vertices[:,1], z=vertices[:,2], i=triangles[:,0], j=triangles[:,1], k=triangles[:,2], facecolor=colors, opacity=0.50)\n",
    "            graph_objects.append(mesh_3d)\n",
    "\n",
    "    fig = go.Figure(\n",
    "        data=graph_objects,\n",
    "        layout=dict(\n",
    "            scene=dict(\n",
    "                xaxis=dict(visible=False),\n",
    "                yaxis=dict(visible=False),\n",
    "                zaxis=dict(visible=False)\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries = draw_geometries # replace function\n",
    "o3d.visualization.draw_geometries([pcd])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "two_view.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
